import os
import sys
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import NearestNeighbors
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
import traceback

import matplotlib.font_manager as fm



# è®¾ç½®matplotlibä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['STSong']
plt.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­
np.random.seed(42)
torch.manual_seed(42)


# ================== ç¬¬ä¸€éƒ¨åˆ†ï¼šæ¨¡å‹å®šä¹‰ä¸åŠ è½½ ==================

class GradientReversalFunction(torch.autograd.Function):
    """æ¢¯åº¦åè½¬å±‚çš„å®ç°"""

    @staticmethod
    def forward(ctx, x, lambda_val):
        ctx.lambda_val = lambda_val
        return x.view_as(x)

    @staticmethod
    def backward(ctx, grad_output):
        output = grad_output.neg() * ctx.lambda_val
        return output, None


class GradientReversalLayer(nn.Module):
    """æ¢¯åº¦åè½¬å±‚å°è£…"""

    def __init__(self, lambda_val=1.0):
        super(GradientReversalLayer, self).__init__()
        self.lambda_val = lambda_val

    def forward(self, x):
        return GradientReversalFunction.apply(x, self.lambda_val)


class DANN(nn.Module):
    """é¢†åŸŸå¯¹æŠ—ç¥ç»ç½‘ç»œæ¨¡å‹"""

    def __init__(self, input_dim=40, num_classes=4, lambda_val=1.0):
        super(DANN, self).__init__()

        # ç‰¹å¾æå–å™¨
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU()
        )

        # æ ‡ç­¾é¢„æµ‹å™¨
        self.label_predictor = nn.Sequential(
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, num_classes)
        )
        # æ¢¯åº¦åè½¬å±‚
        self.grl = GradientReversalLayer(lambda_val)

        # é¢†åŸŸåˆ¤åˆ«å™¨
        self.domain_discriminator = nn.Sequential(
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 1)
        )

    def forward(self, x):
        # æå–ç‰¹å¾
        features = self.feature_extractor(x)

        # æ ‡ç­¾é¢„æµ‹
        class_logits = self.label_predictor(features)

        # é¢†åŸŸåˆ¤åˆ«ï¼ˆé€šè¿‡GRLï¼‰
        reversed_features = self.grl(features)
        domain_logits = self.domain_discriminator(reversed_features)

        return class_logits, domain_logits, features


def load_model_and_data():
    """åŠ è½½æ¨¡å‹å’Œæ•°æ®"""
    print("=" * 80)
    print("åŠ è½½æ¨¡å‹å’Œæ•°æ®...")
    print("=" * 80)

    # 1. åŠ è½½æ¨¡å‹
    model = DANN(input_dim=40, num_classes=4)
    model.load_state_dict(torch.load('dann_model.pth', map_location='cpu'))
    model.eval()
    print("âœ“ æˆåŠŸåŠ è½½æ¨¡å‹æƒé‡: dann_model.pth")

    # 2. åŠ è½½æºåŸŸæ•°æ®
    source_df = pd.read_csv('features.csv')
    feature_columns = [
    "å‡å€¼ (T1)", "å‡æ–¹æ ¹ (T2)", "å³°å€¼ (T3)", "å³°-å³°å€¼ (T4)", "å³­åº¦ (T5)", "æ³¢å½¢æŒ‡æ ‡ (T6)", 
    "å³°å€¼æŒ‡æ ‡ (T7)", "è„‰å†²æŒ‡æ ‡ (T8)", "è£•åº¦æŒ‡æ ‡ (T9)", "å³­åº¦æŒ‡æ ‡ (T10)", "ååº¦ (T11)", 
    "è£‚éš™å› å­(T12)", "è¿‡é›¶ç‡(T13)", "BPFOè°æ³¢å¹…å€¼(f1)", "BPFIè°æ³¢å¹…å€¼(f2)", "BSFè°æ³¢å¹…å€¼(f3)", 
    "è½¬é¢‘å¹…å€¼(f4)", "é¢‘è°±è´¨å¿ƒ(f5)", "è°±å¹³å¦åº¦(f6)", "è°±å¸¦å®½(f7)", "é«˜é¢‘èƒ½é‡å æ¯”(f8)", 
    "é«˜é¢‘å³­åº¦(f9)", "å°æ³¢åŒ…ç‰¹å¾1", "å°æ³¢åŒ…ç‰¹å¾2", "å°æ³¢åŒ…ç‰¹å¾3", "å°æ³¢åŒ…ç‰¹å¾4", 
    "å°æ³¢åŒ…ç‰¹å¾5", "å°æ³¢åŒ…ç‰¹å¾6", "å°æ³¢åŒ…ç‰¹å¾7", "å°æ³¢åŒ…ç‰¹å¾8", "åŒ…ç»œæœ€å¤§å€¼(e1)", 
    "åŒ…ç»œæœ€å°å€¼(e2)", "åŒ…ç»œå‡å€¼(e3)", "åŒ…ç»œæ ‡å‡†å·®(e4)", "åŒ…ç»œå³°åº¦(e5)", "åŒ…ç»œååº¦(e6)", 
    "åŒ…ç»œé¢‘è°±å³°å€¼(e7)", "åŒ…ç»œé¢‘è°±å‡å€¼(e8)", "å¹…å€¼ç†µ(s1)", "è°±ç†µ(s1)"
    ]

    X_source = source_df[feature_columns].values
    y_source = source_df['label'].values
    print(f"âœ“ æˆåŠŸåŠ è½½æºåŸŸæ•°æ®: {len(X_source)} ä¸ªæ ·æœ¬")

    # 3. åŠ è½½ç›®æ ‡åŸŸæ•°æ®
    target_df = pd.read_csv('./features_target.csv')
    X_target = target_df[feature_columns].values
    print(f"âœ“ æˆåŠŸåŠ è½½ç›®æ ‡åŸŸç‰¹å¾: {X_target.shape}")

    # 4. æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_combined = np.vstack([X_source, X_target])
    X_combined_scaled = scaler.fit_transform(X_combined)
    X_source_scaled = X_combined_scaled[:len(X_source)]
    X_target_scaled = X_combined_scaled[len(X_source):]

    # 5. åŠ è½½é¢„æµ‹ç»“æœ
    predictions = pd.read_csv('target_domain_predictions.csv')
    if 'Predicted_Label' not in predictions.columns:
        for col in predictions.columns:
            if 'Label' in col or 'label' in col:
                predictions['Predicted_Label'] = predictions[col]
                break

    print(f"âœ“ æˆåŠŸåŠ è½½é¢„æµ‹ç»“æœ: {len(predictions)} ä¸ªæ ·æœ¬")

    # 6. åŠ è½½t-SNEç‰¹å¾
    tsne_features = np.load('tsne_features_after_transfer.npy')
    print(f"âœ“ æˆåŠŸåŠ è½½å…±äº«ç‰¹å¾: {tsne_features.shape}")

    # ç»„ç»‡æ•°æ®
    source_data = {
        'features': X_source_scaled,
        'features_original': X_source,
        'labels': y_source,
        'shared_features': tsne_features[:len(X_source)]
    }

    target_data = {
        'features': X_target_scaled,
        'features_original': X_target,
        'predictions': predictions,
        'shared_features': tsne_features[len(X_source):]
    }

    print(f"\næ•°æ®åŠ è½½å®Œæˆ:")
    print(f"  æºåŸŸæ ·æœ¬æ•°: {len(X_source)}")
    print(f"  ç›®æ ‡åŸŸæ ·æœ¬æ•°: {len(X_target)}")
    print(f"  ç‰¹å¾ç»´åº¦: {X_source.shape[1]}")

    return model, source_data, target_data, predictions, feature_columns, scaler


# ================== ç¬¬äºŒéƒ¨åˆ†ï¼šåŸºç¡€å¯è§£é‡Šæ€§åˆ†æ ==================

def get_consistent_label_mapping():
    """è·å–ä¸€è‡´çš„æ ‡ç­¾æ˜ å°„å…³ç³»"""
    labels = ['B', 'IR', 'NORMAL', 'OR']
    label_to_idx = {label: idx for idx, label in enumerate(labels)}
    idx_to_label = {idx: label for label, idx in label_to_idx.items()}
    return label_to_idx, idx_to_label


def compute_physics_score_for_hypothesis(sample_features, hypothesis_label, feature_columns):
    """
    è®¡ç®—å‡è®¾æŸä¸ªæ ·æœ¬ä¸ºç‰¹å®šç±»åˆ«æ—¶çš„ç‰©ç†æœºç†ç¬¦åˆåº¦

    Parameters:
    -----------
    sample_features : np.array
        æ ·æœ¬ç‰¹å¾å€¼
    hypothesis_label : str
        å‡è®¾çš„æ•…éšœç±»åˆ«
    feature_columns : list
        ç‰¹å¾åˆ—å

    Returns:
    --------
    score : float
        ç‰©ç†æœºç†ç¬¦åˆåº¦åˆ†æ•°
    analysis_info : dict
        åˆ†æè¯¦æƒ…
    """
    feature_idx_map = {name: idx for idx, name in enumerate(feature_columns)}

    analysis_info = {
        'hypothesis_label': hypothesis_label,
        'key_features': {}
    }

    # ç‰¹å¾æ ‡å‡†åŒ–
    feature_mean = np.mean(sample_features)
    feature_std = np.std(sample_features) + 1e-6
    NORMALized_features = (sample_features - feature_mean) / feature_std

    if hypothesis_label == 'OR':
        bpfo_idx = feature_idx_map.get('BPFO_Amplitude', -1)
        if bpfo_idx >= 0:
            bpfo_NORMALized = NORMALized_features[bpfo_idx]
            score = 1 / (1 + np.exp(-bpfo_NORMALized))
            analysis_info['key_features']['BPFO_Amplitude'] = float(sample_features[bpfo_idx])
            analysis_info['NORMALized_bpfo'] = float(bpfo_NORMALized)
        else:
            score = 0.5

    elif hypothesis_label == 'IR':
        bpfi_idx = feature_idx_map.get('BPFI_Amplitude', -1)
        if bpfi_idx >= 0:
            bpfi_NORMALized = NORMALized_features[bpfi_idx]
            score = 1 / (1 + np.exp(-bpfi_NORMALized))
            analysis_info['key_features']['BPFI_Amplitude'] = float(sample_features[bpfi_idx])
            analysis_info['NORMALized_bpfi'] = float(bpfi_NORMALized)
        else:
            score = 0.5

    elif hypothesis_label == 'B':
        bsf_idx = feature_idx_map.get('BSF_Amplitude', -1)
        if bsf_idx >= 0:
            bsf_NORMALized = NORMALized_features[bsf_idx]
            score = 1 / (1 + np.exp(-bsf_NORMALized))
            analysis_info['key_features']['BSF_Amplitude'] = float(sample_features[bsf_idx])
            analysis_info['NORMALized_bsf'] = float(bsf_NORMALized)
        else:
            score = 0.5

    else:  # NORMAL
        fault_features = ['BPFO_Amplitude', 'BPFI_Amplitude', 'BSF_Amplitude']
        fault_scores = []

        for feat in fault_features:
            feat_idx = feature_idx_map.get(feat, -1)
            if feat_idx >= 0:
                feat_NORMALized = NORMALized_features[feat_idx]
                fault_score = 1 / (1 + np.exp(feat_NORMALized))  # æ³¨æ„è¿™é‡Œæ²¡æœ‰è´Ÿå·
                fault_scores.append(fault_score)
                analysis_info['key_features'][feat] = float(sample_features[feat_idx])

        if fault_scores:
            score = np.mean(fault_scores)
        else:
            score = 0.5

    return float(np.clip(score, 0.0, 1.0)), analysis_info


def compute_domain_alignment_scores(source_data, target_data):
    """è®¡ç®—é¢†åŸŸå¯¹é½è´¨é‡åˆ†æ•°"""
    print("\n" + "=" * 80)
    print("è®¡ç®—é¢†åŸŸå¯¹é½è´¨é‡åˆ†æ•°")
    print("=" * 80)

    source_features = source_data['shared_features']
    source_labels = source_data['labels']
    target_features = target_data['shared_features']
    target_predictions = target_data['predictions']['Predicted_Label'].values

    k_neighbors = 5
    nn_model = NearestNeighbors(n_neighbors=min(k_neighbors, len(source_features)),
                                metric='euclidean')
    nn_model.fit(source_features)

    alignment_scores = []

    for idx in range(len(target_features)):
        target_sample = target_features[idx:idx + 1]
        distances, indices = nn_model.kneighbors(target_sample)
        predicted_label = target_predictions[idx]

        consistent_count = 0
        for neighbor_idx in indices[0]:
            source_label = source_labels[neighbor_idx]
            if source_label == predicted_label:
                consistent_count += 1

        score = consistent_count / len(indices[0])
        alignment_scores.append(score)

    print(f"\né¢†åŸŸå¯¹é½è´¨é‡ç»Ÿè®¡:")
    print(f"  èŒƒå›´: [{min(alignment_scores):.3f}, {max(alignment_scores):.3f}]")
    print(f"  å¹³å‡å€¼: {np.mean(alignment_scores):.3f}")
    print(f"  æ ‡å‡†å·®: {np.std(alignment_scores):.3f}")

    return alignment_scores, nn_model


def compute_classifier_probabilities(model, target_data):
    """è®¡ç®—åˆ†ç±»å™¨çš„åŸå§‹ç½®ä¿¡åº¦"""
    print("\n" + "=" * 80)
    print("è®¡ç®—åˆ†ç±»å™¨åŸå§‹ç½®ä¿¡åº¦")
    print("=" * 80)

    model.eval()
    classifier_probs = []
    all_probabilities = []

    label_to_idx, _ = get_consistent_label_mapping()

    with torch.no_grad():
        X_target_torch = torch.FloatTensor(target_data['features'])
        class_logits, _, _ = model(X_target_torch)
        probabilities = F.softmax(class_logits, dim=1).cpu().numpy()

    for idx, pred_label in enumerate(target_data['predictions']['Predicted_Label']):
        pred_class = label_to_idx.get(pred_label, 0)
        prob = probabilities[idx, pred_class]
        classifier_probs.append(float(prob))
        all_probabilities.append(probabilities[idx])

    print(f"\nåˆ†ç±»å™¨ç½®ä¿¡åº¦ç»Ÿè®¡:")
    print(f"  èŒƒå›´: [{min(classifier_probs):.3f}, {max(classifier_probs):.3f}]")
    print(f"  å¹³å‡å€¼: {np.mean(classifier_probs):.3f}")
    print(f"  æ ‡å‡†å·®: {np.std(classifier_probs):.3f}")

    return classifier_probs, all_probabilities


def perform_basic_analysis(model, source_data, target_data, feature_columns):
    """æ‰§è¡ŒåŸºç¡€å¯è§£é‡Šæ€§åˆ†æ"""
    print("\n" + "=" * 80)
    print("æ‰§è¡ŒåŸºç¡€å¯è§£é‡Šæ€§åˆ†æ")
    print("=" * 80)

    physics_scores = []
    analysis_details = []

    print("è®¡ç®—ç‰©ç†æœºç†ç¬¦åˆåº¦åˆ†æ•°...")

    for idx in tqdm(range(len(target_data['features_original']))):
        sample = target_data['features_original'][idx]
        predicted_label = target_data['predictions'].iloc[idx]['Predicted_Label']

        try:
            score, details = compute_physics_score_for_hypothesis(
                sample, predicted_label, feature_columns)
            physics_scores.append(score)
            analysis_details.append(details)

        except Exception as e:
            print(f"æ ·æœ¬ {idx} ç‰©ç†æœºç†åˆ†æå¤±è´¥: {e}")
            physics_scores.append(0.5)
            analysis_details.append({'error': str(e)})

    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    valid_scores = [s for s in physics_scores if 0 <= s <= 1]
    if valid_scores:
        print(f"\nç‰©ç†æœºç†ç¬¦åˆåº¦ç»Ÿè®¡:")
        print(f"  èŒƒå›´: [{min(valid_scores):.3f}, {max(valid_scores):.3f}]")
        print(f"  å¹³å‡å€¼: {np.mean(valid_scores):.3f}")
        print(f"  æ ‡å‡†å·®: {np.std(valid_scores):.3f}")

    return physics_scores, analysis_details


def compute_comprehensive_confidence(classifier_probs, physics_scores,
                                     alignment_scores, weights=(0.3, 0.5, 0.2)):
    """è®¡ç®—ç»¼åˆè¯Šæ–­ç½®ä¿¡åº¦"""
    print("\n" + "=" * 80)
    print("è®¡ç®—ç»¼åˆè¯Šæ–­ç½®ä¿¡åº¦")
    print("=" * 80)

    w1, w2, w3 = weights
    print(f"\næƒé‡è®¾ç½®:")
    print(f"  åˆ†ç±»å™¨æ¦‚ç‡: {w1 * 100:.0f}%")
    print(f"  ç‰©ç†æœºç†ç¬¦åˆåº¦: {w2 * 100:.0f}%")
    print(f"  é¢†åŸŸå¯¹é½è´¨é‡: {w3 * 100:.0f}%")

    comprehensive_scores = []

    for i in range(len(classifier_probs)):
        score = (w1 * classifier_probs[i] +
                 w2 * physics_scores[i] +
                 w3 * alignment_scores[i])
        comprehensive_scores.append(score)

    print(f"\nç»¼åˆç½®ä¿¡åº¦ç»Ÿè®¡:")
    print(f"  èŒƒå›´: [{min(comprehensive_scores):.3f}, {max(comprehensive_scores):.3f}]")
    print(f"  å¹³å‡å€¼: {np.mean(comprehensive_scores):.3f}")
    print(f"  æ ‡å‡†å·®: {np.std(comprehensive_scores):.3f}")

    return comprehensive_scores


# ================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šäºŒæ¬¡ä¿®æ­£è¯Šæ–­æ¡†æ¶ ==================

def get_all_class_probabilities(model, sample_features):
    """è·å–æ ·æœ¬å¯¹æ‰€æœ‰ç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡"""
    model.eval()
    with torch.no_grad():
        X_tensor = torch.FloatTensor(sample_features)
        class_logits, _, _ = model(X_tensor)
        probabilities = F.softmax(class_logits, dim=1).cpu().numpy()[0]

    class_names = ['B', 'IR', 'NORMAL', 'OR']
    class_probs = {name: prob for name, prob in zip(class_names, probabilities)}

    return class_probs


def compute_alignment_score_for_hypothesis(target_sample_features, hypothesis_label,
                                           source_data, nn_model, k_neighbors=5):
    """è®¡ç®—å‡è®¾æŸä¸ªæ ·æœ¬ä¸ºç‰¹å®šç±»åˆ«æ—¶çš„é¢†åŸŸå¯¹é½è´¨é‡"""
    distances, indices = nn_model.kneighbors(target_sample_features.reshape(1, -1))

    consistent_count = 0
    for neighbor_idx in indices[0]:
        source_label = source_data['labels'][neighbor_idx]
        if source_label == hypothesis_label:
            consistent_count += 1

    score = consistent_count / len(indices[0])
    return float(score)


def compute_comprehensive_confidence_for_hypothesis(classifier_prob, physics_score,
                                                    alignment_score, weights=(0.3, 0.5, 0.2)):
    """è®¡ç®—å‡è®¾ç‰¹å®šç±»åˆ«æ—¶çš„ç»¼åˆç½®ä¿¡åº¦"""
    w1, w2, w3 = weights
    comprehensive_score = w1 * classifier_prob + w2 * physics_score + w3 * alignment_score
    return float(comprehensive_score)


def should_trigger_correction(original_confidence, all_class_probs,
                              uncertainty_threshold=0.6, ambiguity_threshold=0.15):
    """
    åˆ¤æ–­æ˜¯å¦éœ€è¦å¯åŠ¨äºŒæ¬¡ä¿®æ­£

    Parameters:
    -----------
    original_confidence : float
        åŸå§‹ç»¼åˆç½®ä¿¡åº¦
    all_class_probs : dict
        æ‰€æœ‰ç±»åˆ«çš„åˆ†ç±»å™¨æ¦‚ç‡
    uncertainty_threshold : float
        ç½®ä¿¡åº¦é˜ˆå€¼
    ambiguity_threshold : float
        å†³ç­–æ¨¡ç³Šåº¦é˜ˆå€¼ï¼ˆæš‚æ—¶ä¸ä½¿ç”¨ï¼‰

    Returns:
    --------
    should_correct : bool
        æ˜¯å¦éœ€è¦ä¿®æ­£
    reason : str
        è§¦å‘åŸå› 
    """
    # ç®€åŒ–è§¦å‘æ¡ä»¶ï¼šåªè¦ç½®ä¿¡åº¦ä½å°±è§¦å‘äºŒæ¬¡è¯Šæ–­
    trigger_low_confidence = original_confidence < uncertainty_threshold

    # è®¡ç®—å†³ç­–æ¨¡ç³Šåº¦ï¼ˆç”¨äºä¿¡æ¯æ˜¾ç¤ºï¼Œä¸ä½œä¸ºè§¦å‘æ¡ä»¶ï¼‰
    sorted_probs = sorted(all_class_probs.values(), reverse=True)
    ambiguity = sorted_probs[0] - sorted_probs[1] if len(sorted_probs) > 1 else 1.0

    # ä¿®æ”¹åçš„è§¦å‘é€»è¾‘ï¼šåªè¦ä½ç½®ä¿¡åº¦å°±è§¦å‘
    should_correct = trigger_low_confidence

    if should_correct:
        if ambiguity < ambiguity_threshold:
            reason = f"ä½ç½®ä¿¡åº¦({original_confidence:.3f}) + å†³ç­–æ¨¡ç³Š(å·®è·{ambiguity:.3f})"
        else:
            reason = f"ä½ç½®ä¿¡åº¦({original_confidence:.3f})ï¼Œå€¼å¾—äºŒæ¬¡å®¡è§†"
    else:
        reason = f"ç½®ä¿¡åº¦è¶³å¤Ÿ({original_confidence:.3f} â‰¥ {uncertainty_threshold})"

    return should_correct, reason


def validate_correction(original_result, corrected_result, min_improvement=0.02):
    """
    éªŒè¯ä¿®æ­£çš„åˆç†æ€§ - é‡‡ç”¨æ›´å®½æ¾çš„éªŒè¯ç­–ç•¥

    Parameters:
    -----------
    original_result : dict
        åŸå§‹ç»“æœ
    corrected_result : dict
        ä¿®æ­£åç»“æœ
    min_improvement : float
        æœ€å°æ”¹å–„é˜ˆå€¼ï¼ˆè¿›ä¸€æ­¥é™ä½åˆ°0.02ï¼‰

    Returns:
    --------
    is_valid : bool
        ä¿®æ­£æ˜¯å¦æœ‰æ•ˆ
    reason : str
        éªŒè¯åŸå› 
    """
    improvement = corrected_result['comprehensive_confidence'] - original_result['comprehensive_confidence']

    # æ£€æŸ¥1ï¼šæ”¹å–„æ˜¯å¦æ˜¾è‘—ï¼ˆéå¸¸å®½æ¾çš„æ ‡å‡†ï¼‰
    if improvement < min_improvement:
        return False, f"æ”¹å–„ä¸æ˜¾è‘—({improvement:.3f} < {min_improvement})"

    # æ£€æŸ¥2ï¼šç‰©ç†æœºç†æ˜¯å¦åˆç†ï¼ˆè¿›ä¸€æ­¥æ”¾å®½ï¼‰
    if corrected_result['physics_score'] < 0.20:
        return False, f"ç‰©ç†æœºç†è¯„åˆ†è¿‡ä½({corrected_result['physics_score']:.3f})"

    # æ£€æŸ¥3ï¼šé¿å…åŸå§‹ç½®ä¿¡åº¦è¿‡é«˜çš„æ ·æœ¬è¢«ä¿®æ­£ï¼ˆæ–°å¢æ£€æŸ¥ï¼‰
    if original_result['comprehensive_confidence'] > 0.7:
        return False, f"åŸå§‹ç½®ä¿¡åº¦è¾ƒé«˜({original_result['comprehensive_confidence']:.3f})ï¼Œä¸å»ºè®®ä¿®æ­£"

    # æ£€æŸ¥4ï¼šä¸å…è®¸è¿‡åº¦æ¿€è¿›çš„ä¿®æ­£
    if improvement > 0.6:
        return False, f"æ”¹å–„è¿‡äºæ¿€è¿›({improvement:.3f})"

    return True, f"æœ‰æ•ˆä¿®æ­£(æ”¹å–„{improvement:.3f})"


def perform_secondary_correction_analysis(model, source_data, target_data, feature_columns,
                                          original_report, nn_model,
                                          correction_threshold=0.6, weights=(0.3, 0.5, 0.2)):
    """
    æ‰§è¡ŒäºŒæ¬¡ä¿®æ­£è¯Šæ–­åˆ†æ
    """
    print("\n" + "=" * 80)
    print("å¯åŠ¨äºŒæ¬¡ä¿®æ­£è¯Šæ–­æ¡†æ¶ - ä»è§£é‡Šåˆ°ä»²è£")
    print("=" * 80)

    class_names = ['B', 'IR', 'NORMAL', 'OR']
    corrected_report = original_report.copy()
    correction_details = []

    # è¯†åˆ«éœ€è¦åˆ†æçš„æ ·æœ¬
    low_confidence_mask = original_report['Comprehensive_Confidence'] < correction_threshold
    candidate_samples = original_report[low_confidence_mask]

    print(f"å‘ç° {len(candidate_samples)} ä¸ªå€™é€‰æ ·æœ¬ï¼ˆç½®ä¿¡åº¦ < {correction_threshold}ï¼‰")
    print("å¼€å§‹é€ä¸€åˆ†æä¿®æ­£å¯èƒ½æ€§...")

    for idx, row in candidate_samples.iterrows():
        sample_idx = idx
        original_prediction = row['Predicted_Label']
        original_confidence = row['Comprehensive_Confidence']

        # print(f"\nåˆ†ææ ·æœ¬ {row['Filename']} (åŸé¢„æµ‹: {original_prediction}, ç½®ä¿¡åº¦: {original_confidence:.3f})")

        # è·å–æ ·æœ¬ç‰¹å¾
        sample_features_original = target_data['features_original'][sample_idx:sample_idx + 1]
        sample_features_scaled = target_data['features'][sample_idx:sample_idx + 1]
        sample_shared_features = target_data['shared_features'][sample_idx]

        # è·å–æ‰€æœ‰ç±»åˆ«çš„åˆ†ç±»å™¨æ¦‚ç‡
        class_probs = get_all_class_probabilities(model, sample_features_scaled)

        # åˆ¤æ–­æ˜¯å¦éœ€è¦ä¿®æ­£
        should_correct, trigger_reason = should_trigger_correction(
            original_confidence, class_probs, correction_threshold, 0.15)

        if not should_correct:
            # print(f"  è·³è¿‡ä¿®æ­£: {trigger_reason}")
            continue

        # print(f"  è§¦å‘äºŒæ¬¡ä»²è£: {trigger_reason}")

        # ä¸ºæ¯ä¸ªå¯èƒ½çš„ç±»åˆ«è®¡ç®—ç»¼åˆç½®ä¿¡åº¦
        hypothesis_results = {}

        for hypothesis_label in class_names:
            # 1. åˆ†ç±»å™¨æ¦‚ç‡
            classifier_prob = class_probs[hypothesis_label]

            # 2. ç‰©ç†æœºç†ç¬¦åˆåº¦
            physics_score, _ = compute_physics_score_for_hypothesis(
                sample_features_original[0], hypothesis_label, feature_columns)

            # 3. é¢†åŸŸå¯¹é½è´¨é‡
            alignment_score = compute_alignment_score_for_hypothesis(
                sample_shared_features, hypothesis_label, source_data, nn_model)

            # 4. ç»¼åˆç½®ä¿¡åº¦
            comprehensive_score = compute_comprehensive_confidence_for_hypothesis(
                classifier_prob, physics_score, alignment_score, weights)

            hypothesis_results[hypothesis_label] = {
                'classifier_prob': classifier_prob,
                'physics_score': physics_score,
                'alignment_score': alignment_score,
                'comprehensive_confidence': comprehensive_score
            }

            # print(f"    å‡è®¾ {hypothesis_label}: åˆ†ç±»å™¨={classifier_prob:.3f}, "
                #   f"ç‰©ç†={physics_score:.3f}, å¯¹é½={alignment_score:.3f} â†’ ç»¼åˆ={comprehensive_score:.3f}")

        # æ‰¾åˆ°ç½®ä¿¡åº¦æœ€é«˜çš„å‡è®¾
        best_hypothesis = max(hypothesis_results.keys(),
                              key=lambda k: hypothesis_results[k]['comprehensive_confidence'])
        best_result = hypothesis_results[best_hypothesis]
        original_result = hypothesis_results[original_prediction]

        # éªŒè¯ä¿®æ­£çš„åˆç†æ€§
        if best_hypothesis != original_prediction:
            is_valid, validation_reason = validate_correction(original_result, best_result, 0.1)

            if is_valid:
                # print(f"  âœ… æ‰§è¡Œä¿®æ­£: {original_prediction} â†’ {best_hypothesis} ({validation_reason})")

                # æ›´æ–°æŠ¥å‘Š
                corrected_report.loc[idx, 'Predicted_Label'] = best_hypothesis
                corrected_report.loc[idx, 'Classifier_Probability'] = best_result['classifier_prob']
                corrected_report.loc[idx, 'Physics_Mechanism_Score'] = best_result['physics_score']
                corrected_report.loc[idx, 'Domain_Alignment_Score'] = best_result['alignment_score']
                corrected_report.loc[idx, 'Comprehensive_Confidence'] = best_result['comprehensive_confidence']

                # æ›´æ–°ä¸­æ–‡æ ‡ç­¾å’Œç½®ä¿¡åº¦ç­‰çº§
                label_mapping = {
                    'NORMAL': 'æ­£å¸¸',
                    'IR': 'å†…åœˆæ•…éšœ',
                    'OR': 'å¤–åœˆæ•…éšœ',
                    'B': 'æ»šåŠ¨ä½“æ•…éšœ'
                }
                corrected_report.loc[idx, 'æ•…éšœç±»å‹'] = label_mapping[best_hypothesis]

                def get_confidence_level(score):
                    if score >= 0.8:
                        return 'é«˜ç½®ä¿¡åº¦'
                    elif score >= 0.6:
                        return 'ä¸­ç½®ä¿¡åº¦'
                    elif score >= 0.4:
                        return 'ä½ç½®ä¿¡åº¦'
                    else:
                        return 'æä½ç½®ä¿¡åº¦'

                corrected_report.loc[idx, 'Confidence_Level'] = get_confidence_level(
                    best_result['comprehensive_confidence'])

                # è®°å½•ä¿®æ­£è¯¦æƒ…
                correction_details.append({
                    'filename': row['Filename'],
                    'original_prediction': original_prediction,
                    'corrected_prediction': best_hypothesis,
                    'original_confidence': original_confidence,
                    'corrected_confidence': best_result['comprehensive_confidence'],
                    'improvement': best_result['comprehensive_confidence'] - original_confidence,
                    'validation_reason': validation_reason,
                    'all_hypotheses': hypothesis_results
                })
            # else:
                # print(f"  âŒ æ‹’ç»ä¿®æ­£: {validation_reason}")
        # else:
            # print(f"  âœ“ ä¿æŒåŸé¢„æµ‹: æœ€ä¼˜å‡è®¾ä¸åŸé¢„æµ‹ä¸€è‡´")

    return corrected_report, correction_details


def analyze_correction_effectiveness(original_report, corrected_report, correction_details):
    """åˆ†æä¿®æ­£æ•ˆæœ"""
    print("\n" + "=" * 80)
    print("äºŒæ¬¡ä¿®æ­£æ•ˆæœåˆ†æ")
    print("=" * 80)

    print(f"\n1. ä¿®æ­£ç»Ÿè®¡:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(original_report)}")
    print(f"   ä¿®æ­£æ ·æœ¬æ•°: {len(correction_details)}")
    print(f"   ä¿®æ­£æ¯”ä¾‹: {len(correction_details) / len(original_report) * 100:.1f}%")

    if correction_details:
        print(f"\n2. ä¿®æ­£è¯¦æƒ…:")
        for detail in correction_details:
            print(f"   {detail['filename']}: {detail['original_prediction']} â†’ {detail['corrected_prediction']} "
                  f"(+{detail['improvement']:.3f})")

        print(f"\n3. ç½®ä¿¡åº¦æ”¹å–„:")
        original_avg = original_report['Comprehensive_Confidence'].mean()
        corrected_avg = corrected_report['Comprehensive_Confidence'].mean()
        print(f"   åŸå§‹å¹³å‡ç½®ä¿¡åº¦: {original_avg:.3f}")
        print(f"   ä¿®æ­£åå¹³å‡ç½®ä¿¡åº¦: {corrected_avg:.3f}")
        print(f"   æ•´ä½“æ”¹å–„: {corrected_avg - original_avg:.3f}")

        print(f"\n4. ç½®ä¿¡åº¦ç­‰çº§åˆ†å¸ƒå˜åŒ–:")
        original_levels = original_report['Confidence_Level'].value_counts()
        corrected_levels = corrected_report['Confidence_Level'].value_counts()

        all_levels = ['é«˜ç½®ä¿¡åº¦', 'ä¸­ç½®ä¿¡åº¦', 'ä½ç½®ä¿¡åº¦', 'æä½ç½®ä¿¡åº¦']
        for level in all_levels:
            orig_count = original_levels.get(level, 0)
            corr_count = corrected_levels.get(level, 0)
            change = corr_count - orig_count
            if change != 0:
                symbol = "+" if change > 0 else ""
                print(f"   {level}: {orig_count} â†’ {corr_count} ({symbol}{change})")
            else:
                print(f"   {level}: {orig_count} â†’ {corr_count} (æ— å˜åŒ–)")
    else:
        print("\n   æ— æ ·æœ¬è¢«ä¿®æ­£")

    return {
        'correction_count': len(correction_details),
        'correction_rate': len(correction_details) / len(original_report),
        'avg_improvement': corrected_report['Comprehensive_Confidence'].mean() - original_report[
            'Comprehensive_Confidence'].mean()
    }


# ================== ç¬¬å››éƒ¨åˆ†ï¼šæŠ¥å‘Šç”Ÿæˆä¸åˆ†æ ==================

def create_comprehensive_report(target_data, classifier_probs, physics_scores,
                                alignment_scores, comprehensive_scores):
    """åˆ›å»ºç»¼åˆè¯Šæ–­æŠ¥å‘Š"""
    report_df = pd.DataFrame({
        'Filename': target_data['predictions']['Filename'],
        'Predicted_Label': target_data['predictions']['Predicted_Label'],
        'Classifier_Probability': np.round(classifier_probs, 4),
        'Physics_Mechanism_Score': np.round(physics_scores, 4),
        'Domain_Alignment_Score': np.round(alignment_scores, 4),
        'Comprehensive_Confidence': np.round(comprehensive_scores, 4)
    })

    report_df = report_df.sort_values('Comprehensive_Confidence', ascending=False)

    def get_confidence_level(score):
        if score >= 0.8:
            return 'é«˜ç½®ä¿¡åº¦'
        elif score >= 0.6:
            return 'ä¸­ç½®ä¿¡åº¦'
        elif score >= 0.4:
            return 'ä½ç½®ä¿¡åº¦'
        else:
            return 'æä½ç½®ä¿¡åº¦'

    report_df['Confidence_Level'] = report_df['Comprehensive_Confidence'].apply(get_confidence_level)

    label_mapping = {
        'NORMAL': 'æ­£å¸¸',
        'IR': 'å†…åœˆæ•…éšœ',
        'OR': 'å¤–åœˆæ•…éšœ',
        'B': 'æ»šåŠ¨ä½“æ•…éšœ'
    }
    report_df['æ•…éšœç±»å‹'] = report_df['Predicted_Label'].map(label_mapping)

    return report_df


def generate_detailed_analysis_report(original_report, corrected_report, correction_details):
    """ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š"""
    print("\n" + "=" * 80)
    print("ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š")
    print("=" * 80)

    # åˆ›å»ºè¯¦ç»†æŠ¥å‘Š
    detailed_report = corrected_report.copy()

    # æ·»åŠ ä¿®æ­£æ ‡è®°
    detailed_report['Is_Corrected'] = False
    detailed_report['Original_Prediction'] = detailed_report['Predicted_Label']
    detailed_report['Confidence_Improvement'] = 0.0

    for detail in correction_details:
        mask = detailed_report['Filename'] == detail['filename']
        detailed_report.loc[mask, 'Is_Corrected'] = True
        detailed_report.loc[mask, 'Original_Prediction'] = detail['original_prediction']
        detailed_report.loc[mask, 'Confidence_Improvement'] = detail['improvement']

    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    detailed_report.to_csv('detailed_diagnosis_report_with_correction.csv',
                           index=False, encoding='utf-8-sig')

    # ä¿å­˜ä¿®æ­£è¯¦æƒ…
    if correction_details:
        correction_df = pd.DataFrame(correction_details)
        correction_df.to_csv('correction_details.csv', index=False, encoding='utf-8-sig')
        print("ä¿®æ­£è¯¦æƒ…å·²ä¿å­˜ä¸º correction_details.csv")

    print("è¯¦ç»†è¯Šæ–­æŠ¥å‘Šå·²ä¿å­˜ä¸º detailed_diagnosis_report_with_correction.csv")

    return detailed_report


# ================== ç¬¬äº”éƒ¨åˆ†ï¼šä¸»å‡½æ•° ==================

def main():
    """ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„å¯è§£é‡Šæ€§åˆ†æä¸äºŒæ¬¡ä¿®æ­£è¯Šæ–­æµç¨‹"""
    print("\n" * 2)
    print("=" * 80)
    print(" " * 10 + "DANNå¯è§£é‡Šæ€§åˆ†æä¸äºŒæ¬¡ä¿®æ­£è¯Šæ–­æ¡†æ¶")
    print(" " * 15 + "ä»è§£é‡Šåˆ°ä»²è£çš„å®Œæ•´æµç¨‹")
    print("=" * 80)

    try:
        # ç¬¬ä¸€é˜¶æ®µï¼šåŠ è½½æ•°æ®å’Œæ¨¡å‹
        model, source_data, target_data, predictions, feature_columns, scaler = load_model_and_data()

        # ç¬¬äºŒé˜¶æ®µï¼šåŸºç¡€å¯è§£é‡Šæ€§åˆ†æ
        print("\n" + "ğŸ”" + " ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€å¯è§£é‡Šæ€§åˆ†æ")
        physics_scores, analysis_details = perform_basic_analysis(
            model, source_data, target_data, feature_columns)

        alignment_scores, nn_model = compute_domain_alignment_scores(
            source_data, target_data)

        classifier_probs, all_probabilities = compute_classifier_probabilities(
            model, target_data)

        comprehensive_scores = compute_comprehensive_confidence(
            classifier_probs, physics_scores, alignment_scores,
            weights=(0.3, 0.5, 0.2))

        # ç¬¬ä¸‰é˜¶æ®µï¼šç”Ÿæˆåˆå§‹æŠ¥å‘Š
        print("\n" + "ğŸ“Š" + " ç¬¬äºŒé˜¶æ®µï¼šç”Ÿæˆåˆå§‹è¯Šæ–­æŠ¥å‘Š")
        original_report = create_comprehensive_report(
            target_data, classifier_probs, physics_scores,
            alignment_scores, comprehensive_scores)

        # ç¬¬å››é˜¶æ®µï¼šäºŒæ¬¡ä¿®æ­£è¯Šæ–­
        print("\n" + "âš–ï¸" + " ç¬¬ä¸‰é˜¶æ®µï¼šäºŒæ¬¡ä¿®æ­£è¯Šæ–­ä»²è£")

        # å°è¯•ä¸åŒçš„æƒé‡ç»„åˆè¿›è¡ŒäºŒæ¬¡ä¿®æ­£
        weight_schemes = [
            ((0.3, 0.5, 0.2), "æ ‡å‡†æ–¹æ¡ˆï¼šç‰©ç†æœºç†ä¸»å¯¼"),
            ((0.4, 0.4, 0.2), "å¹³è¡¡æ–¹æ¡ˆï¼šåˆ†ç±»å™¨ä¸ç‰©ç†æœºç†å¹³è¡¡"),
            ((0.2, 0.6, 0.2), "ç‰©ç†ä¼˜å…ˆæ–¹æ¡ˆï¼šå¼ºè°ƒç‰©ç†æœºç†"),
            ((0.3, 0.4, 0.3), "å¯¹é½å¼ºåŒ–æ–¹æ¡ˆï¼šé‡è§†è¿ç§»è´¨é‡")
        ]

        best_scheme = None
        best_effectiveness = 0
        all_correction_results = {}

        for weights, scheme_name in weight_schemes:
            print(f"\n{'=' * 60}")
            print(f"å°è¯•æƒé‡æ–¹æ¡ˆ: {scheme_name}")
            print(
                f"æƒé‡åˆ†é…: åˆ†ç±»å™¨{weights[0] * 100:.0f}%, ç‰©ç†æœºç†{weights[1] * 100:.0f}%, é¢†åŸŸå¯¹é½{weights[2] * 100:.0f}%")
            print(f"{'=' * 60}")

            # é‡æ–°è®¡ç®—åŸºäºæ–°æƒé‡çš„åˆå§‹ç»¼åˆç½®ä¿¡åº¦
            scheme_comprehensive_scores = compute_comprehensive_confidence(
                classifier_probs, physics_scores, alignment_scores, weights)

            scheme_original_report = create_comprehensive_report(
                target_data, classifier_probs, physics_scores,
                alignment_scores, scheme_comprehensive_scores)

            # æ‰§è¡ŒäºŒæ¬¡ä¿®æ­£ï¼ˆæé«˜è§¦å‘é˜ˆå€¼åˆ°0.65ï¼‰
            scheme_corrected_report, scheme_correction_details = perform_secondary_correction_analysis(
                model, source_data, target_data, feature_columns,
                scheme_original_report, nn_model, correction_threshold=0.65, weights=weights)

            # åˆ†ææ•ˆæœ
            scheme_effectiveness = analyze_correction_effectiveness(
                scheme_original_report, scheme_corrected_report, scheme_correction_details)

            # è®°å½•ç»“æœ
            all_correction_results[scheme_name] = {
                'weights': weights,
                'original_report': scheme_original_report,
                'corrected_report': scheme_corrected_report,
                'correction_details': scheme_correction_details,
                'effectiveness': scheme_effectiveness
            }

            # é€‰æ‹©æœ€ä½³æ–¹æ¡ˆ
            total_improvement = (scheme_effectiveness['avg_improvement'] +
                                 scheme_effectiveness['correction_rate'] * 0.5)
            if total_improvement > best_effectiveness:
                best_effectiveness = total_improvement
                best_scheme = scheme_name

        # ä½¿ç”¨æœ€ä½³æ–¹æ¡ˆçš„ç»“æœ
        print(f"\n{'ğŸ†' * 60}")
        print(f"æœ€ä½³æƒé‡æ–¹æ¡ˆ: {best_scheme}")
        print(f"{'ğŸ†' * 60}")

        corrected_report = all_correction_results[best_scheme]['corrected_report']
        correction_details = all_correction_results[best_scheme]['correction_details']
        effectiveness = all_correction_results[best_scheme]['effectiveness']
        original_report = all_correction_results[best_scheme]['original_report']

        # ç¬¬äº”é˜¶æ®µï¼šæ•ˆæœåˆ†æ
        print("\n" + "ğŸ“ˆ" + " ç¬¬å››é˜¶æ®µï¼šä¿®æ­£æ•ˆæœåˆ†æ")
        effectiveness = analyze_correction_effectiveness(
            original_report, corrected_report, correction_details)

        # ç¬¬å…­é˜¶æ®µï¼šæ˜¾ç¤ºæœ€ç»ˆæŠ¥å‘Š
        print("\n" + "=" * 80)
        print(" " * 20 + "æœ€ç»ˆè¯Šæ–­æŠ¥å‘Šï¼ˆäºŒæ¬¡ä¿®æ­£åï¼‰")
        print("=" * 80)

        pd.set_option('display.max_columns', None)
        pd.set_option('display.width', None)
        pd.set_option('display.max_colwidth', None)

        print("\næœ€ç»ˆè¯Šæ–­ç»“æœ:")
        display_columns = ['Filename', 'æ•…éšœç±»å‹', 'Classifier_Probability',
                           'Physics_Mechanism_Score', 'Domain_Alignment_Score',
                           'Comprehensive_Confidence', 'Confidence_Level']
        print(corrected_report[display_columns].to_string(index=False))

        # ç¬¬ä¸ƒé˜¶æ®µï¼šç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        print("\n" + "ğŸ“‹" + " ç¬¬äº”é˜¶æ®µï¼šç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š")
        detailed_report = generate_detailed_analysis_report(
            original_report, corrected_report, correction_details)

        # ç¬¬å…«é˜¶æ®µï¼šæ€»ç»“åˆ†æ
        print("\n" + "=" * 80)
        print(" " * 25 + "æ€»ç»“åˆ†æ")
        print("=" * 80)

        print(f"\nğŸ¯ æ ¸å¿ƒæˆæœ:")
        print(f"   â€¢ å»ºç«‹äº†ä»è§£é‡Šåˆ°ä»²è£çš„äºŒæ¬¡è¯Šæ–­æ¡†æ¶")
        print(f"   â€¢ å®ç°äº†å¤šæºä¿¡æ¯èåˆçš„ç½®ä¿¡åº¦è¯„ä¼°")
        print(f"   â€¢ é€šè¿‡ç‰©ç†æœºç†çº¦æŸæå‡äº†è¯Šæ–­å¯ä¿¡åº¦")

        print(f"\nğŸ“Š é‡åŒ–æ•ˆæœ:")
        original_avg = original_report['Comprehensive_Confidence'].mean()
        corrected_avg = corrected_report['Comprehensive_Confidence'].mean()
        print(f"   â€¢ æœ€ä½³æ–¹æ¡ˆ: {best_scheme}")
        print(f"   â€¢ æœ€ä½³æƒé‡: {all_correction_results[best_scheme]['weights']}")
        print(f"   â€¢ å¹³å‡ç½®ä¿¡åº¦: {original_avg:.3f} â†’ {corrected_avg:.3f} (+{corrected_avg - original_avg:.3f})")
        print(f"   â€¢ ä¿®æ­£æ ·æœ¬æ•°: {effectiveness['correction_count']}/{len(original_report)}")
        print(f"   â€¢ ä¿®æ­£æ¯”ä¾‹: {effectiveness['correction_rate'] * 100:.1f}%")

        print(f"\nğŸ”¬ æ–¹æ¡ˆå¯¹æ¯”:")
        for scheme_name, result in all_correction_results.items():
            eff = result['effectiveness']
            weights = result['weights']
            print(
                f"   â€¢ {scheme_name}: æƒé‡{weights} â†’ ä¿®æ­£{eff['correction_count']}ä¸ª, æ”¹å–„{eff['avg_improvement']:+.3f}")

        print(f"\nğŸ”¬ æŠ€æœ¯åˆ›æ–°:")
        print(f"   â€¢ ä»è¢«åŠ¨è§£é‡Šæå‡ä¸ºä¸»åŠ¨ä»²è£")
        print(f"   â€¢ èåˆåˆ†ç±»å™¨æ¦‚ç‡ã€ç‰©ç†æœºç†ã€é¢†åŸŸå¯¹é½ä¸‰ä¸ªç»´åº¦")
        print(f"   â€¢ è®¾è®¡äº†ä¸¥æ ¼çš„ä¿®æ­£è§¦å‘å’ŒéªŒè¯æœºåˆ¶")
        print(f"   â€¢ è‡ªåŠ¨ä¼˜é€‰æœ€ä½³æƒé‡é…ç½®æ–¹æ¡ˆ")

        print(f"\nâš ï¸ å±€é™æ€§è®¤çŸ¥:")
        print(f"   â€¢ ä¿®æ­£æ•ˆæœå—é™äºç‰©ç†æœºç†å»ºæ¨¡çš„å‡†ç¡®æ€§")
        print(f"   â€¢ ç¼ºä¹çœŸå®æ ‡ç­¾éªŒè¯ï¼Œä¿®æ­£å‡†ç¡®æ€§æœ‰å¾…è¿›ä¸€æ­¥éªŒè¯")
        print(f"   â€¢ éœ€è¦åœ¨æ›´å¤§æ•°æ®é›†ä¸ŠéªŒè¯æ¡†æ¶çš„æ³›åŒ–èƒ½åŠ›")

        print("\n" + "=" * 80)
        print(" " * 15 + "å¯è§£é‡Šæ€§åˆ†æä¸äºŒæ¬¡ä¿®æ­£è¯Šæ–­å®Œæˆï¼")
        print("=" * 80)

        return corrected_report, correction_details, effectiveness

    except FileNotFoundError as e:
        print(f"\né”™è¯¯: {e}")
        print("\nè¯·ç¡®ä¿å·²ç»è¿è¡Œç¬¬ä¸‰é—®çš„ä»£ç ï¼Œç”Ÿæˆæ‰€æœ‰å¿…è¦çš„æ–‡ä»¶åå†è¿è¡Œæ­¤åˆ†æã€‚")
        sys.exit(1)

    except Exception as e:
        print(f"\nå‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}")
        print(traceback.format_exc())
        sys.exit(1)


if __name__ == "__main__":

    # æ‰§è¡Œä¸»ç¨‹åº
    final_report, corrections, effectiveness = main()

# python transfer_eval.py